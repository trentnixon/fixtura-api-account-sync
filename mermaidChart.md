# Fixtura Account Sync Service - Human-Readable Data Flow Guide

This document provides a clear, human-friendly overview of how the Fixtura Account Sync service works, with visual diagrams to help understand the system architecture and data flow.

## What This Service Does

The Fixtura Account Sync service is like a digital assistant that keeps sports club and association data up-to-date. It automatically:

- Scrapes competition data from PlayHQ websites
- Processes team and game information
- Stores everything back into the Strapi database
- Handles errors and sends notifications when things go wrong

## High-Level System Overview

```mermaid
graph LR
    A[Strapi Database<br/>ğŸ  Where data lives] --> B[Redis Queue<br/>ğŸ“‹ Job waiting list]
    B --> C[Worker Process<br/>ğŸ‘· Does the work]
    C --> D[PlayHQ Website<br/>ğŸŒ Source of sports data]
    D --> E[Scraped Data<br/>ğŸ“Š Raw information]
    E --> F[Processed Data<br/>âœ¨ Cleaned & organized]
    F --> A

    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e8
    style E fill:#fff8e1
    style F fill:#f1f8e9
```

## The Main Process Flow

Here's how a typical account sync works:

```mermaid
flowchart TD
    Start([ğŸš€ Start: Account needs syncing]) --> Check{What type of account?}

    Check -->|Club| ClubPath[ğŸ¢ Process Club Account]
    Check -->|Association| AssociationPath[ğŸ›ï¸ Process Association Account]

    ClubPath --> GetData[ğŸ“¥ Get account details from database]
    AssociationPath --> GetData

    GetData --> ScrapeComp[ğŸ” Scrape competitions from PlayHQ]
    ScrapeComp --> StoreComp[ğŸ’¾ Store competitions in database]

    StoreComp --> ScrapeTeams[ğŸ” Scrape team data from PlayHQ]
    ScrapeTeams --> StoreTeams[ğŸ’¾ Store teams in database]

    StoreTeams --> ScrapeGames[ğŸ” Scrape game data from PlayHQ]
    ScrapeGames --> StoreGames[ğŸ’¾ Store games in database]

    StoreGames --> UpdateStatus[âœ… Mark account as synced]
    UpdateStatus --> End([ğŸ‰ Complete!])

    %% Error handling
    ScrapeComp -.->|Error| HandleError[âŒ Handle error & notify]
    ScrapeTeams -.->|Error| HandleError
    ScrapeGames -.->|Error| HandleError
    HandleError --> LogError[ğŸ“ Log error details]
    LogError --> NotifySlack[ğŸ“± Send Slack notification]
    NotifySlack --> End

    style Start fill:#e8f5e8
    style End fill:#e8f5e8
    style HandleError fill:#ffebee
    style LogError fill:#fff3e0
    style NotifySlack fill:#e3f2fd
```

## Queue System - How Jobs Are Managed

The service uses Redis queues to manage work efficiently:

```mermaid
graph TD
    subgraph "Job Types"
        A[syncUserAccount<br/>ğŸ”„ Main sync jobs]
        B[onboardNewAccount<br/>ğŸ†• New account setup]
        C[checkAssetGeneratorAccountStatus<br/>ğŸ“Š Asset status check]
    end

    subgraph "Job Processing"
        D[Club Processor<br/>ğŸ¢ Handles club accounts]
        E[Association Processor<br/>ğŸ›ï¸ Handles association accounts]
        F[Onboard Processor<br/>ğŸ†• Handles new accounts]
    end

    subgraph "Job Results"
        G[âœ… Success<br/>Job completed]
        H[âŒ Failed<br/>Job failed]
        I[ğŸ”„ Retry<br/>Try again later]
    end

    A --> D
    A --> E
    B --> F

    D --> G
    D --> H
    E --> G
    E --> H
    F --> G
    F --> H

    H --> I
    I --> A

    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style G fill:#e8f5e8
    style H fill:#ffebee
    style I fill:#fff3e0
```

## Data Processing Pipeline

Here's how raw data becomes useful information:

```mermaid
graph LR
    subgraph "Step 1: Collect"
        A1[ğŸ“¥ Get account info<br/>from database]
        A2[ğŸ“‹ Get team details<br/>from database]
        A3[ğŸ† Get competition info<br/>from database]
    end

    subgraph "Step 2: Scrape"
        B1[ğŸ” Scrape competitions<br/>from PlayHQ]
        B2[ğŸ” Scrape teams<br/>from PlayHQ]
        B3[ğŸ” Scrape games<br/>from PlayHQ]
    end

    subgraph "Step 3: Process"
        C1[ğŸ§¹ Clean competition data<br/>Remove duplicates]
        C2[ğŸ§¹ Clean team data<br/>Validate information]
        C3[ğŸ§¹ Clean game data<br/>Format correctly]
    end

    subgraph "Step 4: Store"
        D1[ğŸ’¾ Save competitions<br/>to database]
        D2[ğŸ’¾ Save teams<br/>to database]
        D3[ğŸ’¾ Save games<br/>to database]
    end

    A1 --> A2 --> A3
    A3 --> B1
    A3 --> B2
    A3 --> B3

    B1 --> C1 --> D1
    B2 --> C2 --> D2
    B3 --> C3 --> D3

    style A1 fill:#e3f2fd
    style A2 fill:#e3f2fd
    style A3 fill:#e3f2fd
    style B1 fill:#e8f5e8
    style B2 fill:#e8f5e8
    style B3 fill:#e8f5e8
    style C1 fill:#fff3e0
    style C2 fill:#fff3e0
    style C3 fill:#fff3e0
    style D1 fill:#f3e5f5
    style D2 fill:#f3e5f5
    style D3 fill:#f3e5f5
```

## Error Handling - When Things Go Wrong

The system is designed to handle errors gracefully:

```mermaid
graph TD
    Error[âŒ Something goes wrong] --> Check{What type of error?}

    Check -->|Network Error| Network[ğŸŒ Can't reach PlayHQ]
    Check -->|Scraping Error| Scraping[ğŸ” Can't extract data]
    Check -->|Database Error| Database[ğŸ’¾ Can't save data]
    Check -->|Queue Error| Queue[ğŸ“‹ Job processing failed]

    Network --> Log[ğŸ“ Log error details]
    Scraping --> Log
    Database --> Log
    Queue --> Log

    Log --> Notify[ğŸ“± Send Slack notification]
    Notify --> Cleanup[ğŸ§¹ Clean up resources]
    Cleanup --> Retry{Should we retry?}

    Retry -->|Yes| Wait[â³ Wait 5 minutes]
    Wait --> RetryJob[ğŸ”„ Try job again]
    RetryJob --> Error

    Retry -->|No| GiveUp[ğŸ›‘ Mark as failed]
    GiveUp --> UpdateStatus[ğŸ“Š Update account status]
    UpdateStatus --> End([ğŸ End process])

    style Error fill:#ffebee
    style Log fill:#fff3e0
    style Notify fill:#e3f2fd
    style Cleanup fill:#f3e5f5
    style RetryJob fill:#e8f5e8
    style GiveUp fill:#ffebee
    style End fill:#e8f5e8
```

## Key Components Explained

### ğŸ  **Strapi Database**

- Stores all account information, teams, competitions, and game data
- Acts as the central hub for all data

### ğŸ“‹ **Redis Queue**

- Manages job scheduling and processing
- Ensures jobs are processed in order
- Handles retries when jobs fail

### ğŸ‘· **Worker Process**

- The main engine that processes jobs
- Coordinates all the different components
- Manages memory and resources

### ğŸŒ **PlayHQ Website**

- External source of sports data
- Provides competition, team, and game information
- Accessed through web scraping

### ğŸ” **Scraping Modules**

- **GetCompetitions**: Finds all competitions for an account
- **GetTeams**: Finds all teams in competitions
- **GetGameData**: Finds all games for teams

### ğŸ’¾ **Assignment Modules**

- **AssignCompetitions**: Saves competition data to database
- **AssignTeams**: Saves team data to database
- **AssignGameData**: Saves game data to database

### ğŸ“ **Logging & Monitoring**

- Tracks all activities and errors
- Sends notifications to Slack
- Monitors memory usage and performance

## How to Read These Diagrams

- **ğŸŸ¢ Green boxes**: Successful operations or data sources
- **ğŸ”µ Blue boxes**: External systems or databases
- **ğŸŸ¡ Yellow boxes**: Processing or transformation steps
- **ğŸ”´ Red boxes**: Errors or failure points
- **ğŸŸ£ Purple boxes**: Storage or persistence operations

The arrows show the flow of data and control through the system. Solid arrows indicate normal flow, while dotted arrows show error handling paths.

## Real-World Example

Let's walk through what happens when a sports club needs to be synced:

### ğŸ¢ **Club Account Sync Example**

1. **ğŸ“‹ Job Created**: Someone adds a new club account to the system
2. **â° Queue Processing**: The job gets added to the `syncUserAccount` queue
3. **ğŸ¢ Club Processing**: The system identifies this as a club account and routes it to the Club Processor
4. **ğŸ“¥ Data Collection**: The system fetches the club's details from the database
5. **ğŸ” Competition Scraping**: It visits the club's PlayHQ page and finds all competitions
6. **ğŸ’¾ Store Competitions**: The competitions are saved to the database
7. **ğŸ” Team Scraping**: For each competition, it finds all teams
8. **ğŸ’¾ Store Teams**: The teams are saved to the database
9. **ğŸ” Game Scraping**: For each team, it finds all games (this can take a while!)
10. **ğŸ’¾ Store Games**: The games are saved to the database
11. **âœ… Complete**: The account is marked as successfully synced

### ğŸ›ï¸ **Association Account Sync Example**

Associations work similarly but handle multiple clubs:

1. **ğŸ“‹ Job Created**: An association account needs syncing
2. **ğŸ›ï¸ Association Processing**: Routed to the Association Processor
3. **ğŸ“¥ Data Collection**: Fetches association details and all its clubs
4. **ğŸ”„ Multiple Clubs**: Processes each club within the association
5. **ğŸ” Competition Scraping**: Finds competitions for the entire association
6. **ğŸ” Team Scraping**: Finds all teams across all clubs
7. **ğŸ” Game Scraping**: Finds all games for all teams
8. **ğŸ’¾ Store Everything**: Saves all data to the database
9. **âœ… Complete**: Association is fully synced

## Common Scenarios

### âœ… **Successful Sync**

- All data is scraped successfully
- Everything is stored in the database
- Account status is updated to "synced"
- Success notification is logged

### âš ï¸ **Partial Failure**

- Some data is scraped successfully
- Some scraping fails (e.g., network issues)
- Successful data is still stored
- Failed parts are logged and may be retried
- Account status reflects partial success

### âŒ **Complete Failure**

- Scraping fails completely
- No data is stored
- Error is logged and Slack notification sent
- Account status is updated to show failure
- Job may be retried later

## Performance Considerations

### ğŸš€ **Speed Optimizations**

- **Batch Processing**: Games are processed in batches of 10 teams
- **Memory Management**: Browser instances are cleaned up after each job
- **Queue Processing**: Multiple jobs can be processed simultaneously
- **Resource Cleanup**: Memory usage is monitored and optimized

### ğŸ“Š **Monitoring**

- **Memory Tracking**: Peak memory usage is recorded for each job
- **Time Tracking**: How long each job takes is logged
- **Error Tracking**: All errors are logged with detailed information
- **Slack Notifications**: Critical errors are sent to Slack immediately

## Troubleshooting Guide

### ğŸ” **Common Issues**

**Problem**: Job keeps failing

- **Check**: Network connection to PlayHQ
- **Check**: Redis queue status
- **Check**: Memory usage (may be too high)
- **Solution**: Restart the worker process

**Problem**: Data not being scraped

- **Check**: PlayHQ website structure (may have changed)
- **Check**: Browser/Puppeteer configuration
- **Check**: Account permissions
- **Solution**: Update scraping selectors

**Problem**: Jobs stuck in queue

- **Check**: Redis connection
- **Check**: Worker process status
- **Check**: Queue configuration
- **Solution**: Clear queue and restart

### ğŸ“ **Logs to Check**

- `combined.log`: General application logs
- `error.log`: Error-specific logs
- Slack notifications: Real-time error alerts
- Memory usage logs: Performance monitoring

This guide should help anyone understand how the Fixtura Account Sync service works, from high-level concepts to specific troubleshooting steps.
